# -*- coding: utf-8 -*-
"""PF - Clasificacion de Imagenes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tRA-SKZCo7O9SxW8a1u6SSw6Mu1-V5pj

![uc3m](http://materplat.org/wp-content/uploads/LogoUC3M.jpg)

Para comenzar a trabajar sobre el notebook de la Práctica Final:

* Complete la siguiente plantilla con los datos del equipo (Número de Equipo, Nombres, NIAs) y súbala a una carpeta en Google Drive con el nombre 'PF_TDI_E[Número de Equipo]_NIA1_NIA2'.
* Comparta la carpeta 'PF_TDI_E[Número de Equipo]_NIA1_NIA2' con los profesores de la asignatura, proporcionándoles permisos de edición: fernando.diaz@uc3m.es, matorres@tsc.uc3m.es, rdmorale@pa.uc3m.es

Borre esta celda de texto una vez haya completado los pasos anteriores.

#### Montar sobre Drive
"""

# Load the Drive helper and mount
from google.colab import drive

# This will prompt for authorization.
drive.mount('/content/drive')

# Change to assignment directory ('TDImagen_Practicas/PF' by default)
import os
os.chdir('/content/drive/My Drive/UC3M/TDI/Practica Final/PF_TDI_E[10]_100346351_100330488')

"""#### Librerías necesarias para la ejecución del software"""

import os
import glob
import numpy  as np
import matplotlib.pyplot as plt # Importa la librería matplotlib
import cv2 # Importa la librería opencv
plt.style.use('default')

### - COMPLETAR CON OTRAS LIBRERIAS SI ES NECESARIO -
import cv2 as cv
from matplotlib.colors import LinearSegmentedColormap 
from pylab import *
from scipy.stats import entropy 
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from mpl_toolkits.mplot3d import Axes3D
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
### - - - - - - - - - - - - - - - - - - - - - - - - -

"""# PRÁCTICA FINAL - CLASIFICACIÓN DE IMÁGENES
---

### DAVID SANCHEZ PRIETO, NIA: 100330488
### MIKEL CRUZ RUIZ, NIA: 100346351

### TRATAMIENTO DIGITAL DE LA IMAGEN

### GRADO EN INGENIERÍA DE SISTEMAS AUDIOVISUALES
### CURSO 2018/2019

1. INTRODUCCIÓN
2. BASE DE DATOS
3. ARQUITECTURA DEL SISTEMA

> 3.1 PREPROCESADO

> 3.2 EXTRACCIÓN DE CARACTERÍSTICAS

> 3.3 CLASIFICACIÓN

4. EVALUACIÓN DE PRESTACIONES

---
## 1. INTRODUCCIÓN
---

El objetivo de esta práctica es explorar los subsistemas de tratamiento de imagen involucrados en un sistema sencillo de clasificación de imágenes. En concreto, se pretende elaborar un prototipo capaz de discriminar imágenes pertenecientes a una determinada categoría de las que no lo son.

---
## 2. BASE DE DATOS
---

Para ello, se trabajará sobre un subconjunto de categorías de las 50 contenidas en la base de datos Animals with Attributes 2, que puede visualizarse y descargarse en su totalidad en el siguiente enlace:

https://cvml.ist.ac.at/AwA2/

En particular nos centraremos en las categorías recogidas en la asignación por equipos. Tanto esta asignación como el archivo .zip con las imágenes correspondientes a cada una de las categorías se encuentran disponibles en Aula Global. Las imágenes pertenecientes a estas clases formarán, por tanto, la base datos de trabajo en este proyecto.

---
## 3. ARQUITECTURA DEL SISTEMA
---

---
### 3.1 PREPROCESADO
---

El primer bloque del sistema tiene el objetivo de mejorar o realzar aquellas regiones y/o propiedades de la imagen para facilitar la etapa de extracción de características. 

En esta etapa del sistema se aplicarán, por tanto, algunas de las técnicas de procesado de imagen vistas en la asignatura (filtrado, detección de bordes, segmentación objeto/fondo, operaciones morfológicas, etc.) con la finalidad de **obtener un conjunto de imágenes **(componente R en el espacio de color RGB, imagen de bordes, máscara binaria de segmentación, etc.) **que permitan extraer descriptores que representen el objeto de la categoría asignada**.
"""

def imageProcessing(image):
  """ La función recibe como entrada la imagen 'image' a
      procesar. A la salida devuelve una variable de tipo
      diccionario ('processed_images') que contiene indexadas
      cada una de las imágenes resultantes de las diferentes
      técnicas de procesado escogidas (imagen filtrada,
      imagen de bordes, máscara de segmentación, etc.), utilizando para cada
      una de ellas una key que identifique su contenido 
      (ej. 'image', 'mask', 'edges', etc.).
      
      Para más información sobre las variables de tipo 
      diccionario en Python, puede consultar el siguiente enlace:
      https://www.w3schools.com/python/python_dictionaries.asp
      
      COMPLETE la función para obtener el conjunto de imágenes procesadas
      'processed_images' necesarias para la posterior extracción de características.
      Si lo necesita, puede definir y hacer uso de funciones auxiliares.
  """
  ### - - - - - - - - - COMPLETAR - - - - - - - - - - - 
  
  ### --- SEGMENTACION PARA BALLENAS OSCURAS --- ###
 
  img_rgb = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)
  #plt.subplot(1, 4, 1)
  #plt.title('Imagen RGB')
  #plt.imshow(img_rgb)
  

  #componente v
  img_hsv = cv2.cvtColor(img_rgb,cv2.COLOR_RGB2HSV)
  img_v = img_hsv[:,:,2]

  #hacemos un filtro de mediana para mejorar la segmentacion
  blur = cv2.medianBlur(img_v, 7)

  #k-means
  Z = blur.reshape((-1,1))
  # convert to np.float32
  Z = np.float32(Z)

  criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)
  ret, label, center	=	cv.kmeans(Z, 3, None, criteria, 10, cv.KMEANS_RANDOM_CENTERS)

  center = np.uint8(center)
  res = center[label.flatten()]
  res2 = res.reshape((blur.shape))

  #centroide
  center1 =center.min() 

  #condicion para establecer máscara binaria
  mask_v = res2
  mask_v[mask_v<center1]=1
  mask_v[mask_v>center1]=0
  
  kernel2 = np.ones((3,3),np.uint8)
  mask_oscuros = cv2.erode(mask_v,kernel2,iterations = 1)
  #plt.subplot(1, 4, 2)
  #plt.title('Mask oscuros')
  #plt.imshow(mask_oscuros, cmap='gray')
  

  ### --- SEGMENTACIÓN PARA BALLENAS CLARAS --- ###
  
  #imagen hsv con filtro mediana
  img_hsv_blur = cv2.medianBlur(img_hsv, 7)
  #imagen rgb con filtro mediana
  img_rgb_blur = cv2.medianBlur(img_rgb, 7)

  #detector de grises para segmentar ese color
  #rango de valores de H 
  Hmin = 80
  Hmax = 140
  #rango de valores de S
  Smin = 0
  Smax = 90
  #rango de valores de V
  Vmin = 0
  Vmax = 170

  gris_bajo = np.array([Hmin, Smin, Vmin], dtype = np.uint8)
  gris_alto = np.array([Hmax, Smax, Vmax], dtype = np.uint8)

  mask = cv2.inRange(img_hsv_blur, gris_bajo, gris_alto)
  mask1 = np.array(img_rgb_blur)

  #seleccionamos una region y se aplica una transformacion
  idx = (mask==0)
  mask1[idx] = 0

  #hacemos una apertura para mejorar el resultado de la mascara
  #erosion
  kernel2 = np.ones((5,5),np.uint8)
  erosion = cv2.erode(mask,kernel2,iterations = 1)
 
  #dilatacion
  kernel2 = np.ones((5,5),np.uint8)
  mask_claros = cv2.dilate(erosion,kernel2,iterations = 1)
  #plt.subplot(1, 4, 3)
  #plt.title('Mask claros')
  #plt.imshow(mask_claros, cmap='gray')
  
  #calculamos porcentaje de pixeles blancos para ambas mascaras
  pix_blanco_claro=np.sum(mask_claros>0)
  pix_blanco_oscuro=np.sum(mask_oscuros>0)
  pix_tot=np.size(img_rgb[:,:,0])
  porcentaje_pix_claro=pix_blanco_claro/pix_tot*100
  porcentaje_pix_oscuro=pix_blanco_oscuro/pix_tot*100
  
  #condicion para seleccionar la mejor segmentacion
  if ((porcentaje_pix_oscuro < porcentaje_pix_claro) or (porcentaje_pix_claro<1.3)):#si hay más % de pix blancos en la mascara de ballenas claras, o dicho porcentaje no supera un umbral minimo (mascara demasiado pequeña como para detecta una ballena) 
      mask_img = mask_oscuros#imprime mascara oscura
  else:
    mask_img = mask_claros
  
  #plt.subplot(1,4,4)
  #plt.title('Mask final')
  #plt.imshow(mask_img, cmap='gray')
  #plt.show()
  
  ### --- SEGMENTACIÓN POR AZUL --- ###
  
  #detector de azules para segmentar ese color
  h_min=90
  h_max=128

  s_min=120
  s_max=255

  v_min=26
  v_max=255
  
  azul_claro=np.array([h_min,s_min,v_min],dtype=np.uint8)
  azul_osc=np.array([h_max,s_max,v_max],dtype=np.uint8)

  mask_a=cv2.inRange(img_hsv,azul_claro,azul_osc)
  #plt.imshow(mask_a,cmap='gray')
  #plt.show()

  kernel = np.ones((3,3),np.uint8)
  mask_blue = cv2.dilate(mask_a,kernel,iterations = 1)
  #plt.imshow(mask_blue,cmap='gray')
  #plt.show()

  #diccionario donde almacenamos las técnicas de procesado
  processed_images = {}
  
  processed_images["img_rgb"] = img_rgb
  processed_images["mask_img"] = mask_img 
  processed_images["mask_blue"] = mask_blue
  
  ### - - - - - - - - - - - - - - - - - - - - - - - - -
  
  return processed_images

### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -

### --- CARGAMOS IMAGENES DE NUESTRO DIRECTORIO --- ###

#directorio_img = sorted(glob.glob('./AwA2TDI/database/blue+whale'+'/*.jpg'))
#print(len(directorio_img))
#for i in range(len(directorio_img)):
#  imag=cv2.imread('.'+'/'+directorio_img[i])
#  imageProcessing(imag)
  
### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -

"""**COMENTE BREVEMENTE AQUÍ LA ETAPA DE PREPROCESADO DE SU SISTEMA:**

Para un análisis con mayor detalle, utilice la memoria a entregar.

Se aconseja utilizar figuras (imágenes, diagramas de dispersión, etc.) para justificar su explicación. Estas figuras pueden ser las mismas que se incluyan en la memoria. Si lo necesita, puede añadir celdas de código y texto adicionales para comentar y visualizar los resultados.

---
### 3.2 EXTRACCIÓN DE CARACTERÍSTICAS
---

Este bloque se encarga de obtener descriptores que sean lo más discriminativos posible, teniendo en cuenta la categoría con la que se trabaja. Cada grupo de trabajo es libre de obtener tantos descriptores como estime conveniente, con un **mínimo obligatorio de tres descriptores**. 

**De estos tres descriptores, al menos uno de ellos ha de apoyarse en las propiedades del objeto asignado**, las cuales se pueden obtener utilizando los archivos clases-tdi.txt, predicates.txt y predicate-matrix-binary-tdi.txt de la base de datos. 

Para más información, consulte el manual de realización de la práctica.
"""

def extractFeatures(processed_images):  
  """ La función recibe como entrada la variable 'processed_images'
      de tipo diccionario, la cual contiene las imágenes procesadas
      obtenidas a partir de cada imagen de la base de datos, necesarias
      para la extracción de las características de dicha imagen. A la salida
      devuelve un vector con los valores de descriptores obtenidos 
      para la imagen.
      
      COMPLETE la función para obtener el vector de características
      escogidas 'features' para representar una imagen de la categoría 
      que se le ha asignado. Si lo necesita, puede definir y hacer uso de 
      funciones auxiliares.
  """
  ### - - - - - - - - - COMPLETAR - - - - - - - - - - - 
  # - - - - - - - - - - - - Listado de descriptores - - - - - - - - - - - - - - #
  
  features = np.zeros(3)
  img_rgb = processed_images["img_rgb"]
  #comp_b = img_rgb[:,:,2]
  img_hsv = cv2.cvtColor(img_rgb,cv2.COLOR_RGB2HSV)
  comp_b = img_hsv[:,:,2]
  #mask1 = np.array(img_rgb)
  #idx = (processed_images["mask_blue"]==0)
  #mask1[idx] = 0
  
  #1. Porcentaje de pixeles azules de la imagen
  cantidad_px_azul=np.mean(comp_b[processed_images["mask_img"]])
  features [0] = cantidad_px_azul
  
  #2. Relación entre el tamaño del objeto segmentado (máscara) y la imagen total
  Hmin_verde=32
  Hmax_verde=75
  
  Smin_verde=38.25
  Smax_verde=255
  
  Vmin_verde=25.5
  Vmax_verde=255
  
  
  verde_bajo=np.array([Hmin_verde, Smin_verde, Vmin_verde], dtype=np.uint8)
  verde_alto=np.array([Hmax_verde, Smax_verde, Vmax_verde], dtype=np.uint8)
  
  
  mask_verde=cv2.inRange(img_hsv,verde_bajo,verde_alto)
  total_px_verdes=np.sum(mask_verde>0)
  total_px_imagen = np.size(img_rgb[:,:,0])
  proporcion_verde=total_px_verdes/total_px_imagen*100
  features[1]=proporcion_verde
  
  #3. Desviación típica de la máscara
  features[2] = np.std(processed_images["mask_img"]>0)
  
  ### - - - - - - - - - - - - - - - - - - - - - - - - -
  
  
  return features

def categoryImages(database_name, category_name):
  """ La función recibe como entrada dos variables de tipo string:
      - 'database_name', que es la ruta a la carpeta que contiene las
      subcarpetas correspondientes a las categorías de la base de datos escogida.
      - 'category_name', que es el nombre de una de las categorías de
      la base de datos.
      
      A su salida, la función devuelve una lista de las imágenes contenidas
      en la categoría seleccionada.
      
      NO es necesario MODIFICAR ni COMPLETAR esta función.
  """
  category_images = sorted(glob.glob(database_name+'/'+category_name+'/*.jpg'))
  return category_images
  
def databaseFeatures(database_name="./AwA2TDI/database_prueba", category_name='blue+whale'):
  """ La función recibe como entrada dos variables:
      - 'database_name', que es la ruta a la carpeta que contiene las
      subcarpetas correspondientes a las categorías de la base de datos escogida.
      - 'category_name', que es el nombre de la categoría de la base de datos
      que se le ha asignado para la realización de la práctica.
      
      A su salida, la función devuelve la matriz de características 'X' y 
      el vector de etiquetas 'y' que representan la base de datos. Estas variables
      se utilizarán en la etapa de clasificación del sistema.
      
      El vector de etiquetas y es binario: asigna un 1 a las imágenes
      que pertenecen a su categoría y un -1 al resto. De esta manera, permite
      resolver un problema de clasificación binaria en el que se discrimine
      su categoría de las del resto de los equipos.
      
      ÚNICAMENTE ES NECESARIO MODIFICAR LA VARIABLE 'num_features' EN
      ESTA FUNCIÓN.
  """
  categories = [ name for name in os.listdir(database_name) if os.path.isdir(os.path.join(database_name, name)) ] 
  print("Estas son las categorías de la base de datos:")
  print(categories)
  
  all_category_images = []
  y = []
  for i in range(len(categories)):
    category_images=categoryImages(database_name, categories[i])
    all_category_images=all_category_images+category_images
    if categories[i]==category_name:
      y=y+list(np.ones(len(category_images)))
    else:
      y=y+list(-np.ones(len(category_images)))

  # Vector de etiquetas y
  y=np.array(y)

  # Matriz de caracteristicas X
  num_features = 3 # MODIFICAR, INDICANDO EL NÚMERO DE CARACTERÍSTICAS EXTRAÍDAS
  X=np.zeros((len(all_category_images),num_features))
  for i in range(len(all_category_images)):
      # OpenCV por defecto carga las imágenes en formato BGR, hay que transformarlas a RGB
      image = cv2.imread(all_category_images[i])
      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
      
      # PREPROCESADO
      processed_images = imageProcessing(image)
      img_rgb = processed_images["img_rgb"]
      
      # EXTRACCION DE CARACTERISTICAS
      X[i,:] = extractFeatures(processed_images)
  
  return X, y

### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -
#Conjuntos de test y train

from sklearn.model_selection import train_test_split

X, y =databaseFeatures()

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.25, random_state=75)
print(np.shape(X_train))
print(np.shape(X_test))

### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -

"""**COMENTE BREVEMENTE AQUÍ LA ETAPA DE EXTRACCIÓN DE CARACTERÍSTICAS DE SU SISTEMA:**

Para un análisis con mayor detalle, utilice la memoria a entregar.

Se aconseja utilizar figuras (imágenes, diagramas de dispersión, etc.) para justificar su explicación. Estas figuras pueden ser las mismas que se incluyan en la memoria. Si lo necesita, puede añadir celdas de código y texto adicionales para comentar y visualizar los resultados.

---
### 3.3 CLASIFICACIÓN
---

**Obtenga un modelo de clasificación binaria que permita separar la categoría que se le ha asignado de las categorías del resto de equipos. **

El clasificador decide, a partir de los descriptores proporcionados por el extractor de características, si la imagen pertenece o no a la categoría de referencia. El clasificador asigna un valor real positivo si la imagen pertenece a la categoría, y negativo en caso contrario. El valor de la salida refleja la confianza en la decisión tomada: un valor absoluto muy elevado indica que el clasificador tiene bastante certeza sobre la pertenencia a la categoría, si es positivo, o sobre su no pertenencia, si es negativo.  

El diseño de un clasificador basado en ejemplos implica una fase de entrenamiento y otra de test. En la primera se ajustan los parámetros del clasificador a partir de un conjunto de imágenes etiquetadas (es decir, imágenes cuya pertenencia o no pertenencia a la clase en cuestión es conocida), y en la segunda se aplica el clasificador a imágenes de test. Por esta razón, el conjunto de imágenes disponibles para el alumno se deberá dividir en dos conjuntos, uno de entrenamiento que se utilizará para ajustar el clasificador, y otro de test que se usará para la evaluación de su calidad. 

Para más información, consulte el manual de realización de la práctica.
"""

def train_classifier(X_train, y_train, X_val = [], y_val = []):
  
  """ La función recibe como entrada:
  
      - Las variables 'X_train', 'y_train', matriz de características
      y vector de etiquetas del conjunto de entrenamiento, respectivamente.
      Permiten obtenener un modelo de clasificación, a escoger por el equipo.
      - Las variables 'X_val', 'y_val', matriz de características y 
      vector de etiquetas del conjunto de validación, respectivamente.
      Permiten validar los parámetros del algoritmo de clasificación escogido.
      Son variables opcionales.
      
      NOTA: Si se desea realizar un procedimiento de validación cruzada del
      modelo, puede subdividir el conjunto de entrenamiento ('X_train','y_train')
      en las particiones necesarias dentro de esta función.
      
      A su salida, la función devuelve el modelo obtenido a partir
      del conjunto de imágenes de entrenamiento.
      
      COMPLETE la función para obtener un modelo de clasificación binaria
      que permita discriminar su categoría de las asignadas al resto de equipos.
      
  """
  best_accuracy=0
  best_n_neighbors=1

  kf = KFold(n_splits=5)
  for n_neighbors in range(1,20):

    neigh=KNeighborsClassifier(n_neighbors)
    validation_sets = kf.split(X_train)

    # Para cada valor de hiperparámetros obtener la precisión utilizando validación cruzada
    suma_accuracy=0
    for train, test in validation_sets:
        Xfold=X_train[train]
        yfold=y_train[train]

        X_val=X_train[test]
        y_val=y_train[test]

        #Entrenamiento del modelo con los datos Xfold e yfold

        model = neigh.fit(Xfold, yfold)

        pred=neigh.predict(X_val)

        #Evaluación del modelo sobre los datos Xval e yval
        accuracy = accuracy_score(pred, y_val)
        suma_accuracy=suma_accuracy+accuracy

    final_accuracy=suma_accuracy/5.0
    if(final_accuracy>best_accuracy):
      best_accuracy=final_accuracy
      best_n_neighbors=n_neighbors
      best_n_neighbors

  
  return model


def test_classifier(model, X_test):
  """ La función recibe como entrada:
  
      - La variable 'model', que contiene el modelo de clasificación binaria obtenido
      mediante la función 'train_classifier' superior.
      - La variable 'X_test', matriz de características del conjunto de test, sobre
      la que se evaluará el modelo de clasificación obtenido a partir del conjunto
      de entrenamiento.
  
      A su salida, la función devuelve el vector de etiquetas predichas
      para las imágenes del conjunto de test. 
      
      COMPLETE la función para obtener el vector de etiquetas predichas 
      'y_pred' para el conjunto 'X_test'.
  """
  y_pred=[]
  y_pred = model.predict_proba(X_test)
  y_pred=y_pred[:,1]

  
  return y_pred

### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -
###
model =train_classifier(X_train,y_train)
y_pred=test_classifier(model,X_test)
###
### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -

"""**COMENTE BREVEMENTE AQUÍ LA ETAPA DE CLASIFICACIÓN DE SU SISTEMA:**

Para un análisis con mayor detalle, utilice la memoria a entregar.

Se aconseja utilizar figuras (imágenes, diagramas de dispersión, etc.) para justificar su explicación. Estas figuras pueden ser las mismas que se incluyan en la memoria. Si lo necesita, puede añadir celdas de código y texto adicionales para comentar y visualizar los resultados.

---
## 4. EVALUACIÓN DE PRESTACIONES
---

**Evalúe y analice las prestaciones del modelo de clasificación obtenido.**

La calidad de un decisor o clasificador binario, cuando se aplica decisión dura, se mide a través de dos parámetros básicos: Probabilidad de detección Pd y probabilidad de falsa alarma Pfa o probabilidad de que una imagen que no pertenece a la categoría sea clasificada de forma errónea como +1. 

Si la decisión es blanda, el área bajo la curva (AUC, Area Under the Curve) proporciona una medida más completa de la calidad del clasificador.

Para más información, consulte el manual de realización de la práctica.
"""

def eval_classifier(y, y_pred):
  """ La función recibe a su entrada el vector de etiquetas
      reales (Ground-Truth (GT)) 'y', así como el vector
      de etiquetas predichas 'y_pred' del conjunto de imágenes
      que se utiliza para evaluar las prestaciones del 
      clasificador.
      
      A su salida, devuelve la variable 'score', con el valor
      obtenido para la medida de evaluación considerada 
      ('AUC', 'accuracy', etc.).
      
      COMPLETE la función para evaluar el clasificador utilizando
      como medida de evaluación, al menos, el Área Bajo la Curva ROC (AUC).
      Si desea obtener otras medidas de evaluación, modifique
      la salida de la función para ello.
  """
  ### - - - - - - - - - COMPLETAR - - - - - - - - - - - 
  score = 0
  score=roc_auc_score(y, y_pred)
  ### - - - - - - - - - - - - - - - - - - - - - - - - -
  
  return score

### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -
model=train_classifier(X_train,y_train)
y_pred=test_classifier(model,X_test)
#print(y_pred)
score=eval_classifier(y_test,y_pred)
print(score)
fpr,tpr,thresshold= roc_curve(y_test,y_pred)
roc_auc=auc(fpr,tpr)
  
plt.title('ROC')
plt.plot(fpr,tpr,'r',label='AUC=%0.2f' %roc_auc)
plt.legend(loc="lower right")
plt.plot([0,1],[0,1],'b--')
plt.xlim([0,1])
plt.ylim([0,1])
plt.ylabel('true positive rate')
plt.xlabel('false positive rate')
plt.show()

fig=plt.figure()
ax=fig.add_subplot(111)
for i in range (len(X_train)):
  ax.scatter(X[y==1,0],X[y==1,1], c='r' , marker='*')
  ax.scatter(X[y==-1,0],X[y==-1,1], c='b' , marker='+')
  ax.set_title("Diagrama de dispersion")
  ax.set_xlabel("Cantidad pixeles azules")
  ax.set_ylabel("Porcentaje pixeles verdes")
  
plt.show()

### - - SI LO NECESITA, ESCRIBA SU CÓDIGO AQUÍ - -

"""**COMENTE BREVEMENTE AQUÍ LOS RESULTADOS OBTENIDOS POR SU SISTEMA:**

Para un análisis con mayor detalle, utilice la memoria a entregar.

Se aconseja utilizar figuras (imágenes, diagramas de dispersión, etc.) para justificar su explicación. Estas figuras pueden ser las mismas que se incluyan en la memoria. Si lo necesita, puede añadir celdas de código y texto adicionales para comentar y visualizar los resultados.
"""